{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb2337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Set env variables for LangSmith (Crucial for observability)\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Enterprise RAG Agent\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "# ... import other necessary modules from LangChain, LangGraph, Chroma, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca700bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in-memory SQLite database with sample sales data\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, Integer, String, Float\n",
    "engine = create_engine('sqlite:///sample_sales.db')\n",
    "metadata = MetaData()\n",
    "\n",
    "# Define a sales table\n",
    "sales_table = Table(\n",
    "    'sales', metadata,\n",
    "    Column('id', Integer, primary_key=True),\n",
    "    Column('product', String),\n",
    "    Column('region', String),\n",
    "    Column('amount', Float),\n",
    "    Column('quarter', Integer)\n",
    ")\n",
    "metadata.create_all(engine)\n",
    "\n",
    "# Insert sample data\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(sales_table.insert(), [\n",
    "        {'product': 'Product A', 'region': 'North', 'amount': 1000.50, 'quarter': 1},\n",
    "        {'product': 'Product B', 'region': 'South', 'amount': 2500.75, 'quarter': 1},\n",
    "        {'product': 'Product A', 'region': 'North', 'amount': 1100.00, 'quarter': 2},\n",
    "        {'product': 'Product C', 'region': 'East', 'amount': 3500.25, 'quarter': 2},\n",
    "        # ... more sample records\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598e9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Load documents (Using Wikipedia for demo, akin to company docs)\n",
    "docs = WikipediaLoader(query=\"Generative artificial intelligence\").load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# Create and persist the vector database\n",
    "vector_db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "vector_db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2d85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# Create a retriever from the vector DB\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})\n",
    "rag_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"search_company_knowledge_base\",\n",
    "    \"Searches and returns information from the company's knowledge base about Generative AI and related topics.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d91a6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.agent_toolkits import create_sql_agent\n",
    "from langchain.utilities import SQLDatabase\n",
    "\n",
    "# Connect to the sample database\n",
    "db = SQLDatabase(engine=engine)\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# This agent itself is a tool that can execute SQL based on natural language\n",
    "sql_agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    db=db,\n",
    "    agent_type=\"openai-tools\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# We need to wrap the agent's execution in a function to make it a LangGraph tool\n",
    "def sql_query_tool(query):\n",
    "    \"\"\"Executes a natural language query on the sales database and returns the results.\"\"\"\n",
    "    return sql_agent.invoke({\"input\": query})\n",
    "\n",
    "# Note: In a full LangGraph implementation, we would use the `Tool` class to wrap this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828880be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "# Define the agent's state\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[HumanMessage], operator.add]\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "# Create a list of tools\n",
    "tools = [rag_tool, sql_query_tool]\n",
    "\n",
    "# Build the graph\n",
    "graph_builder = StateGraph(AgentState)\n",
    "agent = create_react_agent(llm, tools)\n",
    "graph_builder.add_node(\"agent\", agent)\n",
    "graph_builder.set_entry_point(\"agent\")\n",
    "graph_builder.add_edge(\"agent\", END)\n",
    "\n",
    "# Add persistence (important for multi-turn conversations)\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "app = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8426a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Pure RAG Query\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result1 = app.invoke({\"messages\": [HumanMessage(content=\"What is a Variational Autoencoder?\")]}, config=config)\n",
    "print(result1['messages'][-1].content)\n",
    "\n",
    "# Example 2: NL2SQL/BI Query\n",
    "result2 = app.invoke({\"messages\": [HumanMessage(content=\"What were the total sales for Product A in Q1 and Q2? Show me a table.\")]}, config)\n",
    "print(result2['messages'][-1].content)\n",
    "\n",
    "# Example 3: Complex, Multi-Turn Agentic Query\n",
    "# The agent should decide to use both tools\n",
    "result3 = app.invoke({\"messages\": [HumanMessage(content=\"Based on our company knowledge, what are some emerging trends in AI? Also, tell me which product has the highest total sales so I can prioritize investing in those trends.\")]}, config)\n",
    "print(result3['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7883f3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare OpenAI embeddings with a traditional ML method (e.g., PCA on TF-IDF vectors)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample texts\n",
    "texts = [chunk.page_content[:500] for chunk in chunks[:20]] # first 20 chunks\n",
    "\n",
    "# Method 1: OpenAI Embeddings (already have)\n",
    "openai_embeddings = OpenAIEmbeddings().embed_documents(texts)\n",
    "\n",
    "# Method 2: TF-IDF + PCA\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_2d = pca.fit_transform(tfidf_matrix.toarray())\n",
    "\n",
    "# Plot the traditional ML result\n",
    "plt.scatter(tfidf_2d[:, 0], tfidf_2d[:, 1])\n",
    "plt.title(\"Document Similarity using TF-IDF + PCA (Traditional ML)\")\n",
    "for i, txt in enumerate(range(len(texts))):\n",
    "    plt.annotate(txt, (tfidf_2d[i, 0], tfidf_2d[i, 1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
